# -*- coding: utf-8 -*-
"""wavelets_decomp_V3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GWrih26UlrCgKl_55Obgwa9NqvXuLGOE

1. Import libraries and upload data
"""

import numpy as np
import pandas as pd
import pywt
import matplotlib.pyplot as plt
from scipy import ndimage
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from google.colab import files

from google.colab import files
uploaded = files.upload()

"""2. Plot the data"""

df = pd.read_csv("sp500.csv")
df['Date'] = pd.to_datetime(df['Date'])
time = df['Date']
plt.scatter(df["Date"],df["Logreturn"])

"""3. Perform continuous wavelet decomposition on the data and plot it in a scaleogram form"""

y = df['Logreturn']

coef, freqs = pywt.cwt(y[:], scales=np.arange(1,128), wavelet="cmor1.5-1.0", sampling_period=21,method="conv")

coef_float = np.array(coef, dtype = float)

coef_float.shape

plt.matshow(coef_float)
plt.colorbar()
plt.show()
plt.scatter(time[:],y[:])

coef_float.shape

window_size = 21
length = int(df.shape[0])
iteration = length - window_size
print(iteration, length)
range(iteration)

coefficients = []
for i in range(iteration):
  window_end = 21 + i
  coefficients.append(coef_float[i:window_end])

coefficients = np.array(coefficients)
coefficients.shape

"""4. Regime creation using clustering data based on volatility will allow one to use any type of regime identification and have a machine learning model identify regimes in real-time. By reducing the latency of the identification, one can better rebalance to maximize one's portfolio return under each state."""

#Clustering
import statsmodels.tsa.stattools as ts
import seaborn as sns
import sklearn.cluster as cluster
import time
!pip install hdbscan
import hdbscan
from sklearn.cluster import KMeans
import pylab as pl

sns.set_context('poster')
sns.set_color_codes()
plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}


def plot_clusters(data, algorithm, args, kwds):
    start_time = time.time()
    labels = algorithm(*args, **kwds).fit_predict(data)
    end_time = time.time()
    palette = sns.color_palette('deep', np.unique(labels).max() + 1)
    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]
    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)
    frame = plt.gca()
    frame.axes.get_xaxis().set_visible(False)
    frame.axes.get_yaxis().set_visible(False)
    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)
    #plt.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)
    return labels, palette

from google.colab import files
uploaded = files.upload()

df1 = pd.read_csv("regimedata (5).csv")
df1['Date'] = pd.to_datetime(df1['Date'])
df1.index = df1['Date'] #set dates as index for timeseries
del df1['Date']
df1.dropna()

#@title Minimum business days per cluster
df1 = np.array(df1)
cluster_size = 11 #@param {type:"slider", min:0, max:100, step:1}
clusters = []

clusters, colors = plot_clusters(df1, hdbscan.HDBSCAN, (), {'min_cluster_size':cluster_size})     
cluster_labels = set(clusters)

"""5. Use supervised machine learning to train a model to identify regimes in real time"""

from keras.preprocessing.image import ImageDataGenerator
from keras import backend as K
from keras.utils import to_categorical

num_classes = 10
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

# if we are using "channels first" ordering, then reshape the design
# matrix such that the matrix is:
# 	num_samples x depth x rows x columns
if K.image_data_format() == "channels_first":
	x_train = x_train.reshape((x_train.shape[0], 1, 28, 28))
	x_test = x_test.reshape((x_test.shape[0], 1, 28, 28))
 
# otherwise, we are using "channels last" ordering, so the design
# matrix shape should be: num_samples x rows x columns x depth
else:
	x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))
	x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
import time

model = Sequential()
model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) 
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.3))
model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
model.summary()

model.compile(optimizer="adagrad", loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs = 20, validation_data=(x_test, y_test), verbose=1, callbacks=[tensorboard])