# -*- coding: utf-8 -*-
"""maisfinaltest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MoMhinUJ-lsGJdokEUQrVdRbiHEtpGWX

**Dynamic Asset Allocation Across Economic Cycles**
"""

from __future__ import print_function
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, LSTM
from sklearn.decomposition import PCA

from google.colab import files
uploaded = files.upload()

import io
from datetime import datetime
df = pd.read_csv(io.BytesIO(uploaded['Yield_data.csv']))
df['Date'] = pd.to_datetime(df['Date'])
df.index = df['Date'] #set dates as index for timeseries
del df['Date']
df = df.dropna(axis=1, how='all')
df = df.dropna(axis=0, how='all')
TS = pd.DataFrame.copy(df)
df.tail()

# Dataset is now stored in a Pandas Dataframe

col_list = []
for col in df.columns: 
    col_list.append(col)
print(col_list)

# Standardize the Data
from sklearn.preprocessing import StandardScaler

features = col_list

# create standardized df

df = StandardScaler().fit_transform(df)

print(df)

# PCA on macroeconomic data FRED-MD
pca = PCA(n_components = 2)
principalcomponents = pca.fit_transform(df)

pcadf = pd.DataFrame(data = principalcomponents, columns = ['Principal component 1', 'Principal component 2'])

pcadf.head(3)

# Explained Variance from pca model
pca.explained_variance_ratio_

#Visualize PCA data
pcadf.plot.scatter(x='Principal component 1', y = 'Principal component 2')

#Clustering
import statsmodels.tsa.stattools as ts
import seaborn as sns
import sklearn.cluster as cluster
import time
!pip install hdbscan
import hdbscan
from sklearn.cluster import KMeans
import pylab as pl

sns.set_context('poster')
sns.set_color_codes()
plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}


def plot_clusters(data, algorithm, args, kwds):
    start_time = time.time()
    labels = algorithm(*args, **kwds).fit_predict(data)
    end_time = time.time()
    palette = sns.color_palette('deep', np.unique(labels).max() + 1)
    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]
    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)
    frame = plt.gca()
    frame.axes.get_xaxis().set_visible(False)
    frame.axes.get_yaxis().set_visible(False)
    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)
    #plt.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)
    return labels, palette

data = principalcomponents.copy()
plot_clusters(principalcomponents, cluster.KMeans, (), {'n_clusters':6})

#@title Minimum business days per cluster
cluster_size = 21 #@param {type:"slider", min:0, max:100, step:1}

TS.is_copy = False
TS['clusters'], colors = plot_clusters(data, hdbscan.HDBSCAN, (), {'min_cluster_size':cluster_size})     
cluster_labels = set(TS['clusters'])

fig,ax=plt.subplots()
TS[['2Y']].plot(ax = ax, style ='-')

cluster_freq = np.unique(TS['clusters'], return_counts=True)
df_cf_t = pd.DataFrame([cluster_freq[0],cluster_freq[1]])
df_cf = (df_cf_t.transpose()).sort_values([1],ascending = False) 
i = 0
#for cluster_label in cluster_labels:
for i in range(0,top_clusters):
    cluster_label = df_cf.iloc[i][0]
    if (cluster_label >= 0):
        TS[['30Y']][TS['clusters'] == cluster_label].plot(ax = ax, color = colors[cluster_label])
 
        ax.legend_.remove()

